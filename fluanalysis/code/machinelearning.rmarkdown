---
title: "Flu Analysis"
subtitle: "Machine Learning"
author: "Seth Lattner"
output:
  html_document:
    toc: FALSE
---


This file contains data processing and analysis, including some implementation of machine learning. It is conducted on the dataset from McKay et al 2020, found [here](https://doi.org/10.5061/dryad.51c59zw4v).

#### Load and Process Data


```{r, message=FALSE, warning=FALSE}
#load required packages
library(tidyverse)
library(tidymodels)
library(here)
library(rpart)
library(glmnet)
library(ranger)
library(rpart.plot)
library(parallel)
library(vip)
```

```{r}
#load data
flu_data <- readr::read_rds(here("fluanalysis", "data", "flu_data_clean.RDS"))
glimpse(flu_data)
```


I now want to remove a few correlated variables.


```{r}
#remove yes/no variables
flu_data <- flu_data %>%
  select(-ends_with("YN"))

#remove near-zero variance predictors (hearing and vision)
flu_data <- flu_data %>%
  select(-c(Hearing, Vision))

#view new data
glimpse(flu_data)
```


#### Data preparation


```{r}
# set seed for reproducible analysis
set.seed(123) 

#split data into 70% training, 30% testing groups
flu_split <-initial_split(flu_data, prop = 7/10, strata = BodyTemp) 

#New split dataframes 
flu_train <- training(flu_split)
flu_test <- testing(flu_split)
```


Cross validation


```{r}
#5-fold cross-validation, 5 times repeated, stratified by BodyTemp
folds <- vfold_cv(flu_train, v = 5, repeats = 5, strata = BodyTemp)
```


Recipe


```{r, warning=FALSE, message=FALSE}
#recipe for all predictors
global_recipe <- 
  recipe(BodyTemp ~ ., data = flu_train) %>%
  step_dummy(all_nominal(), -all_outcomes())

#null recipe
null_recipe <- 
  recipe(BodyTemp ~ 1, data = flu_train)

#set model engine
linear_reg <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

#create workflow
null_workflow <- workflow() %>%
  add_model(linear_reg) %>%
  add_recipe(null_recipe)

#fit folds data
null_model <- fit_resamples(null_workflow, resamples = folds)

#null model performance
null_performance <- collect_metrics(null_model)
tibble(null_performance)
```


Tree Model


```{r, warning=FALSE, message=FALSE}
#create recipe for tree model
tree_model <- decision_tree( 
  cost_complexity = tune(),
  tree_depth = tune()
  ) %>%
  set_engine("rpart") %>%
  set_mode("regression")

#tree workflow
tree_workflow <- workflow() %>%
  add_model(tree_model) %>%
  add_recipe(global_recipe)

#setup tree tuning grid
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)

#fit to folds data
tree_fit <- tune_grid(tree_workflow, resamples = folds, grid = tree_grid)

#null model performance
tree_performance <- collect_metrics(tree_fit)
tibble(tree_performance)
autoplot(tree_fit)

```

```{r, warning=FALSE, message=FALSE}
#select best tree model
best_tree <- tree_fit %>%
  select_best(metric = 'rmse')

#final model
final_tree <- tree_workflow %>%
  finalize_workflow(best_tree)

#fit final model to data
tree_fit_final <- final_tree %>%
  fit(flu_train)

#plot final fit
rpart.plot::rpart.plot(extract_fit_parsnip(tree_fit_final)$fit)

#tree residual plot
tree_pred <- tree_fit_final %>%
  fit(flu_train) %>%
  predict(flu_train)

tree_resid <- flu_train$BodyTemp - tree_pred$.pred
tree_df <- tibble(tree_resid, tree_pred)

ggplot(aes(.pred, tree_resid), data = tree_df)+
  geom_point()+
  labs(x = "Predictions", y = "Residuals")+
  geom_hline(yintercept = 0)+
  theme_bw()
```


LASSO


```{r}
#LASSO model
lasso_model <- linear_reg(
  penalty=tune(),mixture=1) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

#LASSO workflow
lasso_workflow <- workflow() %>%
  add_model(lasso_model) %>%
  add_recipe(global_recipe)

#LASSO grid
lasso_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

#highest and lowest penalty values
lasso_grid %>%
  top_n(5)
lasso_grid %>%
  top_n(-5)

#model tuning and training
lasso_train <- lasso_workflow %>%
  tune_grid(
    resamples=folds,
    grid=lasso_grid,
    control=control_grid(save_pred = TRUE),
    metrics=metric_set(rmse))

lasso_train %>%
  collect_metrics()

autoplot(lasso_train)

#select best model
lasso_best <- lasso_train %>%
  select_best(metric = 'rmse')

#finalized lasso 
lasso_final <- lasso_workflow %>%
  finalize_workflow(lasso_best)

lasso_fit <- lasso_final %>% 
  fit(flu_train)

lasso_pred <- lasso_final %>%
  fit(flu_train) %>%
  predict(flu_train)

plot_fit <- extract_fit_engine(lasso_fit)
plot(plot_fit, "lambda")

lasso_resid <- flu_train$BodyTemp - lasso_pred$.pred
lasso_df <- tibble(lasso_resid, lasso_pred)

ggplot(aes(.pred, lasso_resid), data = lasso_df)+
  geom_point()+
  labs(x = "Predictions", y = "Residuals")+
  geom_hline(yintercept = 0)+
  theme_bw()

```


Random Forest


```{r}
#create cores object
cores <- parallel::detectCores()

#rf model
rf_model <- rand_forest(mtry=tune(),min_n=tune(),trees=1000) %>%
  set_engine("ranger", num.threads = cores)%>%
  set_mode("regression")

#rf workflow
rf_workflow <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(global_recipe)

rf_train <- rf_workflow %>%
  tune_grid(
    resamples=folds,
    grid=25,
    control=control_grid(save_pred=TRUE),
    metrics = metric_set(rmse))

autoplot(rf_train)

rf_train %>% collect_metrics()

#select best rf model
rf_best <- rf_train %>%
  select_best(metric = 'rmse')

#final rf model
rf_final <- rf_workflow %>% 
  finalize_workflow(rf_best)

rf_fit <- rf_final %>% 
  fit(flu_train)

rf_pred <- rf_final %>%
  fit(flu_train) %>%
  predict(flu_train)

rf_resid <- flu_train$BodyTemp - rf_pred$.pred
rf_df <- tibble(rf_resid, rf_pred)

ggplot(aes(.pred, rf_resid), data = rf_df)+
  geom_point()+
  labs(x = "Predictions", y = "Residuals")+
  geom_hline(yintercept = 0)+
  theme_bw()
```


Compare model performances


```{r}
null_rmse <- null_model %>% show_best(metric = "rmse")
null_rmse # 1.207

tree_rmse <- tree_fit %>% show_best(metric = "rmse", n=1)
tree_rmse # 1.189

lasso_rmse <- lasso_train %>% show_best(metric = "rmse", n=1)
lasso_rmse # 1.155

rf_rmse <- rf_train %>% show_best(metric = "rmse", n=1)
rf_rmse # 1.163

```


The RMSE values for the best performing models were as follows:

| Null  | Tree  | LASSO | Random Forest |
|-------|-------|-------|---------------|
| 1.207 | 1.189 | 1.155 | 1.163         |

Based on these RMSE values, the LASSO appears to have the best performance. Because of this, this model will be used to fit the test data.



```{r}
final_model <- lasso_final %>% 
  last_fit(split = flu_split, metrics = metric_set(rmse))

final_model %>% collect_metrics()


#plot of fit
final_pred <- final_model %>% 
  collect_predictions() 
final_model %>% 
  extract_fit_engine() %>% 
  vip()

# residual plot
final_resid <- final_pred$BodyTemp - final_pred$.pred
final_df <- tibble(final_resid, final_pred)

ggplot(aes(.pred, final_resid), data = final_df)+
  geom_point()+
  labs(x = "Predictions", y = "Residuals")+
  geom_hline(yintercept = 0)+
  theme_bw()

```

